{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part III (25 pts total)\n",
        "---\n",
        "\n",
        "**SN:** 15066212\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *29th March, 2022, - EC apply*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part3.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will be guided through the implementation of a full deep reinforcement learning agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVBcO5mAV9Ow"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run all the cells in this section, but do not modify them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1p0fpbxQLyn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lpweIqAWBX3"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIRC73HLq6VH"
      },
      "source": [
        "# A) Actor-critics\n",
        "\n",
        "You are going to implement an Actor-critic agent that updates a policy parametrised as a deep neural network.\n",
        "\n",
        "The agent learns online from a single stream of experience, updating the parametes of its policy after each transition in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV03Q3MpveUM"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "wc-kqp3tveUT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jaxlib in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (0.3.0)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.3.2-cp38-none-macosx_10_9_x86_64.whl (70.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 70.0 MB 36.7 MB/s eta 0:00:01    |█████▌                          | 11.9 MB 1.4 MB/s eta 0:00:42 |███████▎                        | 15.8 MB 1.4 MB/s eta 0:00:39     |████████████████████████▌       | 53.7 MB 61.7 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib) (2.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib) (1.20.1)\n",
            "Requirement already satisfied: scipy in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib) (1.6.2)\n",
            "Requirement already satisfied: absl-py in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib) (1.0.0)\n",
            "Requirement already satisfied: six in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from absl-py->jaxlib) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.0\n",
            "    Uninstalling jaxlib-0.3.0:\n",
            "      Successfully uninstalled jaxlib-0.3.0\n",
            "Successfully installed jaxlib-0.3.2\n",
            "fatal: destination path 'bsuite' already exists and is not an empty directory.\n",
            "Processing ./bsuite\n",
            "Requirement already satisfied: absl-py in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (1.0.0)\n",
            "Requirement already satisfied: dm_env in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (1.5)\n",
            "Requirement already satisfied: immutabledict in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (3.4.3)\n",
            "Requirement already satisfied: numpy in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (1.20.1)\n",
            "Requirement already satisfied: pandas in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (1.2.4)\n",
            "Requirement already satisfied: plotnine in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (0.8.0)\n",
            "Requirement already satisfied: scipy in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (1.6.2)\n",
            "Requirement already satisfied: scikit-image in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (0.18.1)\n",
            "Requirement already satisfied: six in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from bsuite==0.3.5) (1.1.0)\n",
            "Requirement already satisfied: dm-tree in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from dm_env->bsuite==0.3.5) (0.1.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->bsuite==0.3.5) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->bsuite==0.3.5) (1.3.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->bsuite==0.3.5) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->bsuite==0.3.5) (2.8.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->bsuite==0.3.5) (8.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from pandas->bsuite==0.3.5) (2021.1)\n",
            "Requirement already satisfied: statsmodels>=0.12.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from plotnine->bsuite==0.3.5) (0.12.2)\n",
            "Requirement already satisfied: descartes>=1.1.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from plotnine->bsuite==0.3.5) (1.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from plotnine->bsuite==0.3.5) (0.5.1)\n",
            "Requirement already satisfied: mizani>=0.7.3 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from plotnine->bsuite==0.3.5) (0.7.3)\n",
            "Requirement already satisfied: palettable in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from mizani>=0.7.3->plotnine->bsuite==0.3.5) (3.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->bsuite==0.3.5) (2.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->bsuite==0.3.5) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->bsuite==0.3.5) (2020.10.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from scikit-image->bsuite==0.3.5) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from networkx>=2.0->scikit-image->bsuite==0.3.5) (5.0.6)\n",
            "Building wheels for collected packages: bsuite\n",
            "  Building wheel for bsuite (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for bsuite: filename=bsuite-0.3.5-py3-none-any.whl size=250307 sha256=fc70b2fac6dc75ba0295c56a0ab96ee9dd0a9c2e4aaaa216ea1f017610c70603\n",
            "  Stored in directory: /private/var/folders/rl/xtsx604x2y976k2p6f67bymc0000gn/T/pip-ephem-wheel-cache-k_09v6ss/wheels/11/d0/8c/96425607848ed365869040924633f975a9c5799dc205b89861\n",
            "Successfully built bsuite\n",
            "Installing collected packages: bsuite\n",
            "  Attempting uninstall: bsuite\n",
            "    Found existing installation: bsuite 0.3.5\n",
            "    Uninstalling bsuite-0.3.5:\n",
            "      Successfully uninstalled bsuite-0.3.5\n",
            "Successfully installed bsuite-0.3.5\n",
            "Requirement already satisfied: dm-haiku==0.0.6 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (0.0.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from dm-haiku==0.0.6) (0.8.9)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from dm-haiku==0.0.6) (0.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from dm-haiku==0.0.6) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from dm-haiku==0.0.6) (1.20.1)\n",
            "Requirement already satisfied: six in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from absl-py>=0.7.1->dm-haiku==0.0.6) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U jaxlib\n",
        "!git clone https://github.com/deepmind/bsuite.git\n",
        "!pip install bsuite/\n",
        "!pip install dm-haiku==0.0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "q442-kGUvTf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optax in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (0.1.1)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from optax) (0.3.2)\n",
            "Requirement already satisfied: chex>=0.0.4 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from optax) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from optax) (1.20.1)\n",
            "Requirement already satisfied: jax>=0.1.55 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from optax) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from optax) (4.1.1)\n",
            "Requirement already satisfied: six in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from absl-py>=0.7.1->optax) (1.15.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from chex>=0.0.4->optax) (0.11.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax>=0.1.55->optax) (1.6.2)\n",
            "Requirement already satisfied: opt-einsum in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: rlax in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (0.1.2)\n",
            "Requirement already satisfied: distrax>=0.0.2 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from rlax) (0.1.1)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from rlax) (1.0.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from rlax) (0.3.2)\n",
            "Requirement already satisfied: jax>=0.1.55 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from rlax) (0.3.2)\n",
            "Requirement already satisfied: chex>=0.0.8 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from rlax) (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from rlax) (1.20.1)\n",
            "Requirement already satisfied: six in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from absl-py>=0.9.0->rlax) (1.15.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from chex>=0.0.8->rlax) (0.11.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from chex>=0.0.8->rlax) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from distrax>=0.0.2->rlax) (0.16.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax>=0.1.55->rlax) (1.6.2)\n",
            "Requirement already satisfied: opt-einsum in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax>=0.1.55->rlax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax>=0.1.55->rlax) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib>=0.1.37->rlax) (2.0)\n",
            "Requirement already satisfied: decorator in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-probability>=0.15.0->distrax>=0.0.2->rlax) (5.0.6)\n",
            "Requirement already satisfied: gast>=0.3.2 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-probability>=0.15.0->distrax>=0.0.2->rlax) (0.5.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-probability>=0.15.0->distrax>=0.0.2->rlax) (1.6.0)\n",
            "Requirement already satisfied: jax==0.3.2 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (0.3.2)\n",
            "Requirement already satisfied: absl-py in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax==0.3.2) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax==0.3.2) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax==0.3.2) (4.1.1)\n",
            "Requirement already satisfied: opt-einsum in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax==0.3.2) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jax==0.3.2) (1.6.2)\n",
            "Requirement already satisfied: six in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from absl-py->jax==0.3.2) (1.15.0)\n",
            "Collecting jaxlib==0.1.74\n",
            "  Downloading jaxlib-0.1.74-cp38-none-macosx_10_9_x86_64.whl (59.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.6 MB 14.9 MB/s eta 0:00:01 MB 1.7 MB/s eta 0:00:34  | 13.5 MB 1.7 MB/s eta 0:00:28     |██████████                      | 18.8 MB 7.9 MB/s eta 0:00:06\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib==0.1.74) (2.0)\n",
            "Requirement already satisfied: absl-py in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib==0.1.74) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib==0.1.74) (1.20.1)\n",
            "Requirement already satisfied: scipy in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from jaxlib==0.1.74) (1.6.2)\n",
            "Requirement already satisfied: six in /Users/joanrossello/opt/anaconda3/lib/python3.8/site-packages (from absl-py->jaxlib==0.1.74) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.2\n",
            "    Uninstalling jaxlib-0.3.2:\n",
            "      Successfully uninstalled jaxlib-0.3.2\n",
            "Successfully installed jaxlib-0.1.74\n"
          ]
        }
      ],
      "source": [
        "# solution\n",
        "!pip install optax\n",
        "!pip install rlax\n",
        "!pip install jax==0.3.2\n",
        "!pip install --upgrade jaxlib==0.1.74"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_pTfi5dSFX5"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "huyKrYpvSHSu"
      },
      "outputs": [],
      "source": [
        "from bsuite.environments import catch\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "QCWBfrXLjM4p"
      },
      "outputs": [],
      "source": [
        "# solution\n",
        "import optax\n",
        "import rlax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6kEki4XHbPy"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "EYdWwRrbHbcl"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(list_of_episode_returns):\n",
        "  \"\"\"Plot the learning curve.\"\"\"\n",
        "  plt.figure(figsize=(7, 5))\n",
        "\n",
        "  def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "  smoothed_returns = moving_average(list_of_episode_returns, 30)\n",
        "  plt.plot(smoothed_returns)\n",
        "\n",
        "  plt.xlabel('Average episode returns')\n",
        "  plt.xlabel('Number of episodes')\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.spines['left'].set_visible(True)\n",
        "  ax.spines['bottom'].set_visible(True)\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.xaxis.set_ticks_position('bottom')\n",
        "  ax.yaxis.set_ticks_position('left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtlrr5d2p7cS"
      },
      "source": [
        "### Neural networks\n",
        "\n",
        "You will use JAX to define a network parametrising:\n",
        "\n",
        "* The values of each state $v(s)$.\n",
        "* The action preferences in each state $\\{p_i(s)\\}_{i\\in\\{1, ..., |A|\\}}$ (you can think of and implement the preferences $\\mathbf{p}(s)$ as a vector output with $|A|$ elements).\n",
        "\n",
        "You will use `Haiku` (https://github.com/deepmind/dm-haiku) to define the network. You will need to:\n",
        "* define the forward pass of the network as some function `fn`\n",
        "* Use `hk.transform(fn)` to convert this in a pair of functions `init_net` and `apply_net`):\n",
        "\n",
        "\n",
        "        init_net, apply_net = hk.transform(fn)\n",
        "\n",
        "The `init` function has signature `parameters = init_net(key, obs)`, where\n",
        "  * `key` is a Jax random-number-generator key that we will generate with `jax.random.PRNGKey`,\n",
        "  * `observation` is the observation of the agent, which will be flattened into a vector by the network, as described below.\n",
        "\n",
        "The `init_net` returns randomly sampled weights for the neural network:\n",
        "\n",
        "        parameters = init_net(jax.random.PRNGKey(1234), observation)\n",
        "\n",
        "The `apply_net` will have signature `v, p = apply_net(parameters, current_key, obs)` \n",
        "  * `parameters`: the (current) parameters of the ageent,\n",
        "  * `key`: a Jax random keym, e.g., generated with `current_key, key = jax.random.split(key)` (we then keep `key`, which is also a new key, around to split again the next time we need a random key),\n",
        "  * `observation`: the observation of the agent.\n",
        "\n",
        "The `apply_net` function then returns a **scalar** value `v` and a **vector** of preferences `p`, which will define the policy of the agent.\n",
        "\n",
        "A lot of the boilerplate code has been written for you.  You just have to implement some functions as indicated in the questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj1nsUYuOoe1"
      },
      "source": [
        "### Q1 [2 marks]\n",
        "\n",
        "Define the forward pass of the neural network. The network must:\n",
        "\n",
        "* take an `observation` as input\n",
        "* reshape the observation into a flat vector `flat_obs`\n",
        "* compute a hidden representation `h = relu(W.dot(flat_obs) + b)`, where `relu(x)` is a 'rectifier linear unit', which computes $\\max(x, 0)$ elementwise (you can just use `jax.nn.relu(x)` to implement this),\n",
        "* compute a vector of action preferences as a linear function of `h`\n",
        "* compute a scalar state value as a linear function of `h`\n",
        "* return the scalar value and vector preferences.\n",
        "\n",
        "Note:\n",
        "* The hidden representation should be a vector of 50 elements.\n",
        "* The action preferences should be a vector of 3 elements (one per each available action).\n",
        "* The value should be a scalar (not a vector with one element).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "vRBP0xjtQvTi"
      },
      "outputs": [],
      "source": [
        "def forward_pass(observation):\n",
        "  # Implement forward pass here\n",
        "  # What we are basically doing here is:\n",
        "  # 1) We flatten the observations in a single vedctor of whatever size\n",
        "  # 2) Then this is linearly connected to a hidden layer of 50 nodes\n",
        "  # 3) We apply a relu activation on the hidden layer\n",
        "  # 4) Then we have a linear connection from the hidden layer to the output layer, which has just 3 nodes (i.e. preferences of each action)\n",
        "  \n",
        "  network = hk.Sequential([\n",
        "    lambda x: jax.numpy.reshape(x, (-1)),\n",
        "    hk.Linear(50),\n",
        "    jax.nn.relu\n",
        "  ])\n",
        "\n",
        "  obs = network(observation)\n",
        "\n",
        "  v = hk.Linear(1)(obs) # decoder 2 -- weights\n",
        "  v = np.squeeze(v)\n",
        "  v = v[()] # the value should be a scalar, not a vector with one element\n",
        "  \n",
        "  p = hk.Linear(3)(obs) # decoder 1 -- thetas\n",
        "  \n",
        "  return v, p\n",
        "\n",
        "# Our forward pass will be deterministic, so we apply `hk.without_apply_rng`.\n",
        "init_net, apply_net = hk.without_apply_rng(hk.transform(forward_pass))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ImJUgzFosvD"
      },
      "source": [
        "### Choosing actions\n",
        "\n",
        "A critical component of an actor-critic agent is a (stochastic) policy, mapping `observations` to `actions`. \n",
        "\n",
        "In deep RL, this mapping is conventionally parametrised by a deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MVXNWwlYW24"
      },
      "source": [
        "### Q2 [2 marks]\n",
        "\n",
        "Implement a softmax policy parametrised by the neural network above (i.e., using the `apply_net` function).\n",
        "\n",
        "The function has signature `action = softmax_policy(parameters, key, obs)`, taking the current network parameters `parameters`, a JAX random `key` and the current `observation`. It should return an `action` sampled from a softmax distribution, so that the probability of selecting action $a$ with preference $p(s, a)$ is proportional to $\\exp(p(s, a))$.\n",
        "\n",
        "Functions to perform random sampling in JAX (e.g. those in `jax.random`) take a random key as input, and they are deterministic function of such a key. In general, in a JAX program you need to use the `jax.random.split` function to generate new random keys before every new sampling. The run loop that runs the experiment later on splits the key between consecutive calls to the `policy`. Thus you can assume that a new random `key` is provided to you on each call to the `policy`. If, however, you find that you need a random key in multiple places within the `policy`, do remember to split the key before each use, using `key1, key2 = jax.random.split(key)`.\n",
        "\n",
        "Note that we 'jit' the function.  This means the function will be compiled, which will make it run faster. This does also suppress print statements, so if you are debugging and want to print please comment out the `@jax.jit` line, but don't forget to put it back before running the experiment below, and especially before submitting your assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "x-FSOYNOYnUx"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def softmax_policy(parameters, key, obs):\n",
        "  \"\"\"Sample action from a softmax policy.\"\"\"\n",
        "  _, p = apply_net(parameters, obs)\n",
        "\n",
        "  # Create the list of actions to choose from:\n",
        "  actions = list(range(len(p)))\n",
        "  actions = jax.numpy.array(actions) # turn it into jax array\n",
        "\n",
        "  # The probability of each action in 'a' is defined by 'p', which is the softmax of the action preferences\n",
        "  # To calculate the softmax probabilities we use the jax function\n",
        "  action = jax.random.choice(key=key, a=actions, p=jax.nn.softmax(p))\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMYcb7Y9krnT"
      },
      "source": [
        "### Learning values and policies\n",
        "\n",
        "An actor-critic agent requires to update the parameters of the network so as to simultaneously improve the value predictions and the policy.\n",
        "\n",
        "In the next section you will define the gradient updates for each of these two components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsIlpmNEk5fv"
      },
      "source": [
        "### Q3 [4 marks]\n",
        "\n",
        "Implement a function to compute a stochastic estimate of the policy gradient from a 1 step transition in the environment.\n",
        "\n",
        "* You will use $R_{t+1} + \\gamma v(S_{t+1})$ as an estimate of $q_{\\pi}(S_t, A_t)$\n",
        "* You will use $v(S_{t})$ as a baseline to reduce the variance of the updates.\n",
        "\n",
        "In the code we actually use names `obs_tm1`, `a_tm1` (where `tm1` stands for '$t$ minus one') for the observation and action at time $t-1$, and `r_t`, `discount_t`, `obs_t` for the reward, discount, and observation at time $t$. So the code is offset a single time step in terms of naming as compared to the typical mathematical formulations.  This is just a naming convention, and should not impact the algorithm.\n",
        "\n",
        "The function below must therefore have signature `grads = policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)`.\n",
        "* Where the inputs are:\n",
        "  * `parameters`: the parameters of the network,\n",
        "  * an observation `obs_tm1`\n",
        "  * the action `a_tm1` selected after observing `obs_tm1`,\n",
        "  * the resulting reward `r_t` and discount `discount_t` and observation `obs_t`, as obsesrved after taking action `a_tm1`.\n",
        "\n",
        "This function should return a stochastic estimate of the policy gradient, where `grads` has the same structure as `parameters` and contains an estimate of the gradient of the expected episodic return wrt to each parameter.\n",
        "\n",
        "The policy-gradient estimate should use bootstrapping, using the value estimates that can be gotten using the saame `parameters` as used for the policy.  So the output of this function can be used in a one-step actor-critic update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02RJAKiX85Wx"
      },
      "source": [
        "\n",
        "### Jax hint:\n",
        "Note that you can use `jax.grad(f)` to get the gradient of any (pure) jax function with a scalar output.  For instance, consider:\n",
        "\n",
        "        def f(w, x, y):\n",
        "          # w, x, and y are all vectors\n",
        "          return jnp.sum(w*x + y)\n",
        "\n",
        "        df = jax.grad(f)\n",
        "    \n",
        "then calling `df(w, x, y)` will give the gradient of the output of `f(w, x, y)` with respect to the first input argument --- here called `w`. You can use this new function `df` as just a normal function. For instance, it can be called from other functions, as usual.\n",
        "\n",
        "Run the cell below to see a concrete example in action.  Note that `df(w, x, y)` in the example below evaluates to the same values as `x`.  This is correct, because the gradient of `f` with respect to `w` is indeed `x`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "t1TjveqqGnNA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f(w, x, y):  31.0\n",
            "df(w, x, y): [3. 5.]\n"
          ]
        }
      ],
      "source": [
        "def f(w, x, y):\n",
        "  return jnp.sum(w*x + y)\n",
        "\n",
        "df = jax.grad(f)\n",
        "\n",
        "w = jnp.array([1., 2.])\n",
        "x = jnp.array([3., 5.])\n",
        "y = jnp.array([7., 11.])\n",
        "\n",
        "print(f'f(w, x, y):  {f(w, x, y)}')\n",
        "print(f'df(w, x, y): {df(w, x, y)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wngNEiwrQeF0"
      },
      "source": [
        "### Jax hint:\n",
        "if you have a Haiku network with parameters `w`, then these parameters will typically be a structured dictionary containing the actual weight vectors and matrices that make up the network's parameters.  Suppose you have a function that computes the output of a network, and then uses this to do some stuff, and you want the gradient of that function:\n",
        "\n",
        "        # Define network\n",
        "        def some_haiku_net(...):\n",
        "          ...\n",
        "\n",
        "        # Transform to get init and apply\n",
        "        init, apply = hk.without_apply_rng(hk.transform(some_haiku_net))\n",
        "\n",
        "        # Define a new function\n",
        "        def some_function(w, x, y):\n",
        "           ...some stuff...\n",
        "           # compute output of the network:\n",
        "           output = apply(w, x)\n",
        "           ...some more stuff that uses output...\n",
        "           return ...some_scalar...\n",
        "        \n",
        "        grad_function = jax.grad(some_function)\n",
        "\n",
        "As before, `grads = grad_function(w, x, y)` should now give the gradients of `some_function` with respect to the first input argument, `w`.  These gradients `grads` will have exactly the same shape as the input argment `w`.  But we cannot do things like `w + grads`, because `w` and `grads` are both dictionaries, not just `jnp.array`s.  Instead, we can use Jax' inbuilt tree utils.  For instance, to add the gradients to the weights, we can use:\n",
        "`new_weights = jax.tree_map(lambda w_i, g_i: w_i + 0.1 * g_i, w, grads)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "AHLGjy8he6vh"
      },
      "outputs": [],
      "source": [
        "def policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "\n",
        "  v_tm1, _ = apply_net(parameters, obs_tm1)\n",
        "  v_t, _ = apply_net(parameters, obs_t)\n",
        "\n",
        "  def policy(param, obs, a):\n",
        "    _, p = apply_net(param, obs)\n",
        "    return jax.numpy.log(jax.nn.softmax(p)[a])\n",
        "\n",
        "  delta = r_t + discount_t * v_t - v_tm1\n",
        "\n",
        "  grad_function = jax.grad(policy)\n",
        "\n",
        "  grads = jax.tree_map(lambda g: delta * g, grad_function(parameters, obs_tm1, a_tm1))\n",
        "  \n",
        "  return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XgjefrBlIGN"
      },
      "source": [
        "### Q4 [4 marks]\n",
        "\n",
        "Implement a function to compute a TD(0) update for the parameters of the value function.\n",
        "\n",
        "It must have signature `td_update = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)`.\n",
        "* Where the inputs are:\n",
        "  * the current parameters `parameters` of the network,\n",
        "  * an observation `obs_tm1`\n",
        "  * the action `a_tm1` selected after observing `obs_tm1`,\n",
        "  * the resulting reward `r_t` and environment discount `discount_t` \n",
        "  * and the following observation `obs_t`\n",
        "* Returns a stochastic TD(0) semi-gradient update: `td_update` has the same structure as `parameters`. This contains a stochastic estimate of the negative semi-gradient of the expected value prediction loss: a TD(0) update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "CAONu72iTpzx"
      },
      "outputs": [],
      "source": [
        "def value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  \n",
        "  v_tm1, _ = apply_net(parameters, obs_tm1)\n",
        "  v_t, _ = apply_net(parameters, obs_t)\n",
        "\n",
        "  def v_w(param, obs):\n",
        "    v, _ = apply_net(param, obs)\n",
        "    return v\n",
        "\n",
        "  delta = r_t + discount_t * v_t - v_tm1\n",
        "\n",
        "  v_grad = jax.grad(v_w)\n",
        "\n",
        "  # td_update = delta * v_grad(parameters, obs_tm1)\n",
        "  td_update = jax.tree_map(lambda g: delta * g, v_grad(parameters, obs_tm1))\n",
        "\n",
        "  return td_update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXgLZg8mfMd"
      },
      "source": [
        "### Updating shared parameters\n",
        "\n",
        "The policy gradient identifies the direction of change in the parameters that most steeply improve the policy.\n",
        "The value update identifies the direction of change in the parameters that improves the value predictions (according to TD).\n",
        "\n",
        "However, the value and policy share some of the parameters of the network.  How do we combine the two gradient updates?\n",
        "\n",
        "In this assignment, we simply sum the policy and value components.\n",
        "The function that combines the two gradients is implemented for you in the cell below.  Note the use of `jax.tree_multimap` to facilitate adding the structured parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "uhKCLe8jjkdZ"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def compute_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pgrads = policy_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  td_update = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  return jax.tree_multimap(lambda pg, td: pg + td, pgrads, td_update)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYaVb6GcpCRe"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "In deep learning, gradient updates are typically rescaled and modifed to avoid taking too large a step on a single update (e.g., due to large variance), and to facilitate the optimisation process (it turns out raw stochatic gradients are often not the most effective for updating neural networks).\n",
        "\n",
        "For instance given a candidate gradient update $\\nabla$ we may update our parameters $\\theta$ by;\n",
        "$$\\Delta \\theta = \\theta + \\alpha * \\nabla\\,,$$\n",
        "where $\\alpha$ is a small number between 0 and 1 (e.g., $\\alpha=0.01$ or $\\alpha=0.001$), referred to as `step_size` or `learning_rate`\n",
        "\n",
        "The gradients with respect to each weight of a neural network may however have very different magnitudes. This can make it hard to set a suitable learning rate $\\alpha$.\n",
        "\n",
        "In deep learning, and deep RL, we typically use adaptive learning rates, for instance by rescaling each component of the gradient using statistics tracking the typical size of the updates to that weight. Then the entire update is rescaled using a global `learning_rate` $\\alpha$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbFePWfApaMU"
      },
      "source": [
        "### Q5 [2 marks]\n",
        "\n",
        "A popular approach to adaptive gradient rescaling was introduced by the `Adam` algorithm.\n",
        "This algorithm implements the following procedure before applying each update:\n",
        "* Increase an update counter $k \\gets k+1$ (starting at k=0 before any updates),\n",
        "* Update the first moment of each gradient component $\\mu \\gets (1 - \\beta_1) g + \\beta_1 \\mu$ where $g$ is the latest stochastic gradient, where $\\beta_1$ is a parameter for the moving average.\n",
        "* Update the second moment of each gradient component $\\nu_i = (1 - \\beta_2) g_i ^ 2 + \\beta_2 \\nu_i $ where $g$ is the latest gradient update, where $\\beta_2$ is a parameter for the moving average.\n",
        "* Use the following update to update the weights:\n",
        "$$\\Delta w = \\alpha \\frac{\\mu / (1 - \\beta_1 ^ {k})}{\\epsilon + \\sqrt{\\nu / (1 - \\beta_1 ^ {k})}}$$\n",
        "* $\\alpha$ is a global `learning rate`\n",
        "* $\\beta_1$ and $\\beta_2$ define a soft horizon for the per-weight statistics.\n",
        "* $\\epsilon$ makes the rescaling more robust to numerical issues.\n",
        "\n",
        "(See [Kingma et al, 2014](https://arxiv.org/abs/1412.6980) for details, if you are interested.)\n",
        "\n",
        "In the next cell define a pair of functions (`opt_init`, and `opt_update` --- in each case `opt` is short for 'optimiser'), where:\n",
        "\n",
        "The `opt_init` function has signature `opt_state = opt_init(parameters)`.\n",
        "* Takes the network parameters as inputs\n",
        "* Initialises an `optimiser state` holding the per weight statistics.\n",
        "\n",
        "The `opt_update` function has signature `updates, opt_state = opt_update(grads, opt_state)`.\n",
        "* Takes a `gradient` and an `optimisers state`,\n",
        "* and returns the transformed gradient and the updated `optimiser state`.\n",
        "\n",
        "The optimiser state `opt_state` should contain:\n",
        "* The first-order momentum $\\mu$, as updated with a moving-average-parameter $\\beta_1$ which we call `b1` in the code.\n",
        "* The first-order momentum $\\nu$, as updated with a moving-average-parameter $\\beta_2$ which we call `b2` in the code.\n",
        "\n",
        "We will ignore the initial correction Adam typically uses, and will instead use the simpler transformation:\n",
        "$$\\Delta w = \\alpha \\frac{\\mu}{\\epsilon + \\sqrt{\\nu}}$$\n",
        "\n",
        "Set the algorithm's hyper-parameters to $\\alpha=0.003$, $\\beta_1=.9$ and $\\beta_2=.999$, $\\epsilon=10^{-4}$.  You are allowed to hard-code these in, or make them configurable (e.g., pass them as additional arguments to `opt_update`, which is better if you want to play around, of course).  Set the initial moving averages to zero for $\\mu$ and one for $\\nu$.  E.g.,\n",
        "\n",
        "        mu = jax.tree_multimap(jnp.zeros_like, parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "017XRA_BpbZx"
      },
      "outputs": [],
      "source": [
        "def opt_init(parameters):\n",
        "\n",
        "  mu = jax.tree_multimap(jnp.zeros_like, parameters)\n",
        "  veta = jax.tree_multimap(jnp.ones_like, parameters)\n",
        "\n",
        "  opt_state = [mu, veta]\n",
        "\n",
        "  return opt_state\n",
        "\n",
        "# Note that we have a gradient for each parameter, and with each gradient we compute the parameter update for each parameter.\n",
        "# mu and veta have the same size as the parameters, and consequently, as the gradients.\n",
        "\n",
        "def opt_update(grads, opt_state):\n",
        "  # Hard-code hyperparameters:\n",
        "  alpha = 0.003\n",
        "  b1 = 0.9\n",
        "  b2 = 0.999\n",
        "  eps = 1e-4\n",
        "\n",
        "  # Get mu and veta from opt_state:\n",
        "  mu = opt_state[0]\n",
        "  veta = opt_state[1]\n",
        "\n",
        "  # Update mu and veta:\n",
        "  # mu = (1 - b1) * grads + b1 * mu\n",
        "  mu = jax.tree_map(lambda g, m: (1 - b1) * g + b1 * m, grads, mu)\n",
        "  # veta = (1 + b2) * grads**2 + b2 * veta\n",
        "  veta = jax.tree_map(lambda g, v: (1 - b2) * g**2 + b2 * v, grads, veta)\n",
        "\n",
        "  # updates = alpha * (mu / (eps + veta**(0.5)))\n",
        "  updates = jax.tree_map(lambda m, v: alpha * m / (eps + jax.numpy.sqrt(v)), mu, veta)\n",
        "\n",
        "  opt_state = [mu, veta]\n",
        "\n",
        "  return updates, opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create environment.\n",
        "# env = catch.Catch(seed=42)\n",
        "\n",
        "# # Build and initialize network.\n",
        "# rng = jax.random.PRNGKey(44)\n",
        "# rng, init_rng = jax.random.split(rng)\n",
        "# sample_input = env.observation_spec().generate_value()\n",
        "# param = init_net(init_rng, sample_input)\n",
        "\n",
        "# m = jax.tree_multimap(jnp.ones_like, param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DczWvZfNSnTj"
      },
      "source": [
        "### Run experiments\n",
        "\n",
        "Run the cell below to show the performance of the resulting agent.\n",
        "\n",
        "You may also use this section for debugging your implementations.\n",
        "\n",
        "Note however, that most functions are `jitted` for performance,\n",
        "* either using the `@jax.jit` decorator in the function definition\n",
        "* or calling explicitely `fn = jax.jit(fn)`\n",
        "\n",
        "When jitting, the code is compiled on the first time the function is executed\n",
        "* and execution is much faster on subsequent calls.\n",
        "* a notable side effect is that print statements in a jitted function will only execute on the first execution of the function.\n",
        "* to drop into a debugger or print on each function execution you will have to disable the `@jax.jit` annotations and jax.jit calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "Fz837XTkLxE8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training agent for 1500 episodes...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "\n",
        "# Experiment configs.\n",
        "train_episodes = 1500\n",
        "discount_factor = .99\n",
        "\n",
        "# Create environment.\n",
        "env = catch.Catch(seed=42)\n",
        "\n",
        "# Build and initialize network.\n",
        "rng = jax.random.PRNGKey(44)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "sample_input = env.observation_spec().generate_value()\n",
        "parameters = init_net(init_rng, sample_input)\n",
        "\n",
        "# Initialize optimizer state.\n",
        "opt_state = opt_init(parameters)\n",
        "\n",
        "# Apply updates\n",
        "def apply_updates(params, updates):\n",
        "  return jax.tree_multimap(lambda p, u: p + u, params, updates)\n",
        "\n",
        "# Jit.\n",
        "opt_update = jax.jit(opt_update)\n",
        "apply_updates = jax.jit(apply_updates)\n",
        "\n",
        "print(f\"Training agent for {train_episodes} episodes...\")\n",
        "all_episode_returns = []\n",
        "\n",
        "for _ in range(train_episodes):\n",
        "  episode_return = 0.\n",
        "  timestep = env.reset()\n",
        "  obs_tm1 = timestep.observation\n",
        "\n",
        "  # Sample initial action.\n",
        "  rng, policy_rng = jax.random.split(rng)\n",
        "  a_tm1 = softmax_policy(parameters, policy_rng, obs_tm1)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Step environment.\n",
        "    new_timestep = env.step(int(a_tm1))\n",
        "\n",
        "    # Sample action from agent policy.\n",
        "    rng, policy_rng = jax.random.split(rng)\n",
        "    a_t = softmax_policy(parameters, policy_rng, new_timestep.observation)\n",
        "\n",
        "    # Update params.\n",
        "    r_t = new_timestep.reward\n",
        "    discount_t = discount_factor * new_timestep.discount\n",
        "    dJ_dtheta = compute_gradient(\n",
        "        parameters, obs_tm1, a_tm1, r_t, discount_t,\n",
        "        new_timestep.observation)\n",
        "    updates, opt_state = opt_update(dJ_dtheta, opt_state)\n",
        "    parameters = apply_updates(parameters, updates)\n",
        "\n",
        "    # Within episode book-keeping.\n",
        "    episode_return += new_timestep.reward\n",
        "    timestep = new_timestep\n",
        "    obs_tm1 = new_timestep.observation\n",
        "    a_tm1 = a_t\n",
        "\n",
        "  # Experiment results tracking.\n",
        "  all_episode_returns.append(episode_return)\n",
        "\n",
        "# Plot learning curve.\n",
        "plot_learning_curve(all_episode_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dowFJ_l-32A"
      },
      "source": [
        "# B) An alternative update\n",
        "\n",
        "You are going to implement a different kind of agent.\n",
        "\n",
        "Like an actor-critic, it learns online from a single stream of experience, updating the parametes after each transition in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQD0Qw8-_QJy"
      },
      "source": [
        "### Neural networks\n",
        "\n",
        "\n",
        "The agent will reuse the same neural network we defined for the actor-critic:\n",
        "* the scalar output will be trained via TD to estimate state values\n",
        "* the vector preferences `p` will be updated according to a different rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6j0AIF8GhdR"
      },
      "source": [
        "### Choosing actions\n",
        "\n",
        "As in actor-critics, the (stochastic) mapping from `observations` to `actions` depends on the vector of preferences `p` from the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE00LhDL_nbC"
      },
      "source": [
        "### Q6 [2 marks]\n",
        "\n",
        "The new agent's policy will have the signature `action = epsilon_greedy_policy(parameters, key, observation)`,\n",
        "* Take as inputs the current network parameters `parameters`, a JAX random `key` and the current `observation`\n",
        "* Return with probability `0.9` the greedy `action` with respect to the preferences `p`, ties must be broken at random.\n",
        "* Return an action uniformly at random with probability `0.1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "id": "8IwTDROL_Lx5"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def epsilon_greedy_policy(parameters, key, observation):\n",
        "    \"\"\"Sample action from an epsilon-greedy policy.\"\"\"\n",
        "    _, p = apply_net(parameters, observation)\n",
        "    e = 0.1\n",
        "\n",
        "    key1, key2 = jax.random.split(key)\n",
        "\n",
        "    # Create the list of actions to choose from:\n",
        "    actions = list(range(len(p)))\n",
        "    actions = jax.numpy.array(actions) # turn it into jax array\n",
        "\n",
        "    n_actions = len(p)\n",
        "\n",
        "    # In order to use jax.random.choice here, we need to find a way to express the two alternatives of:\n",
        "    # 1) choosing any action with probability epsilon\n",
        "    # 2) choosing the greedy action with probability 1-epsilon\n",
        "    # So we choose the non-greedy actions eps/n.actions, and we choose the greedy action 1-eps+eps/n.actions, since\n",
        "    # when we are not being greedy, we might still choose the greedy action with equal probability wrt the others.\n",
        "\n",
        "    # First we need to find the greedy action, which will be the one with highest preference value in the vector p\n",
        "    # If there is a tie, we need to break it randomly.\n",
        "\n",
        "    p_max = jax.numpy.max(p)\n",
        "    \n",
        "    # ------------ note ---------------\n",
        "    # In this part, we should be using the function 'flatnonzero' as shown in the commented line below, to find ALL the \n",
        "    # actions that have the maximum preference. However, jax keeps getting an error when I use this function or try the same\n",
        "    # with 'argwhere', and I cannot manage to find why, because when I try it outside of this cell, it works fine.\n",
        "\n",
        "    # a_max = jax.numpy.flatnonzero(p == p_max) # select all actions that have maximum value of preference\n",
        "\n",
        "    # Instead, we use 'argmax', which simply outputs the first action that has maximum preference, so then random tiebreaking\n",
        "    # does not make sense, since we always have one since action anyways, but we have coded the random tiebreaking because it is\n",
        "    # what we would do if 'flatnonzero' worked.\n",
        "    a_max = jax.numpy.argmax(p, keepdims=True)\n",
        "    # ---------end of note -------------\n",
        "\n",
        "    greedy_action = jax.random.choice(key=key1, a=a_max) # select one action randomly (each with equal probability)\n",
        "\n",
        "    # Now we create the probabilities for each non-greedy action, and the greedy action, as stated above:\n",
        "    probab = jax.numpy.where(actions == greedy_action, 1-e+e/n_actions, e/n_actions)\n",
        "\n",
        "    # Now we are ready to select the action using jax.random.choice:\n",
        "    action = jax.random.choice(key=key2, a=actions, p=probab)\n",
        "        \n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1zQiRX5BfeA"
      },
      "source": [
        "### Q7 [3 marks]\n",
        "\n",
        "The parameters $w_p$ of the preferences $p_{w_p}(s, a)$ will be update according to the following gradient-based update:\n",
        "\n",
        "$$\\Delta w_p = \\alpha (R_{t+1} + \\gamma v(S_{t+1}) - p(S_{t}, A_t)) \\nabla p(S_t, A_t))$$\n",
        "\n",
        "where `v` is the state value trained by TD as in the actor critic.\n",
        "\n",
        "You musy implement this in the function `preference_gradient`, with the same signature as `policy_gradient`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "id": "6fQhAWXqLNt-"
      },
      "outputs": [],
      "source": [
        "def preference_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  \n",
        "  v_t, _ = apply_net(parameters, obs_t)\n",
        "\n",
        "  def pref(param, obs, a):\n",
        "    _, p = apply_net(param, obs)\n",
        "    return p[a]\n",
        "\n",
        "  delta = r_t + discount_t * v_t - pref(parameters, obs_tm1, a_tm1)\n",
        "\n",
        "  grad_function = jax.grad(pref)\n",
        "\n",
        "  # grads = delta * grad_function(parameters, obs_tm1, a_tm1)\n",
        "  grads = jax.tree_map(lambda g: delta * g, grad_function(parameters, obs_tm1, a_tm1))\n",
        "\n",
        "  return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz4RFKnVF8E8"
      },
      "source": [
        "### Updating shared parameters\n",
        "\n",
        "Just like in the actor critic the overall update to the parameters is a combination of two quantities:\n",
        "* the new update we defined for the vector of preferences\n",
        "* the same TD update to the scalar output that we used in the actor critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "QnghCzKhF-gI"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def compute_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pgrads = preference_gradient(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  vgrads = value_update(parameters, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  return jax.tree_multimap(lambda pg, td: pg + td, pgrads, vgrads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44QBKEF4HFK8"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "The gradient updates are rescaled using the same optimiser used for the actor-critic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbdHQo23FJit"
      },
      "source": [
        "### Run experiments\n",
        "\n",
        "Run the cell below to show the performance of the new agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {
        "id": "ept5NG1oFLnu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training agent for 15000 episodes...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "\n",
        "# Experiment configs.\n",
        "train_episodes = 15000\n",
        "discount_factor = .99\n",
        "\n",
        "# Create environment.\n",
        "env = catch.Catch(seed=42)\n",
        "\n",
        "# Build and initialize network.\n",
        "rng = jax.random.PRNGKey(44)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "sample_input = env.observation_spec().generate_value()\n",
        "parameters = init_net(init_rng, sample_input)\n",
        "\n",
        "# Initialize optimizer state.\n",
        "opt_state = opt_init(parameters)\n",
        "\n",
        "# Apply updates\n",
        "def apply_updates(params, updates):\n",
        "  return jax.tree_multimap(lambda p, u: p + u, params, updates)\n",
        "\n",
        "# Jit.\n",
        "opt_update = jax.jit(opt_update)\n",
        "apply_updates = jax.jit(apply_updates)\n",
        "\n",
        "print(f\"Training agent for {train_episodes} episodes...\")\n",
        "all_episode_returns = []\n",
        "\n",
        "for _ in range(train_episodes):\n",
        "  episode_return = 0.\n",
        "  timestep = env.reset()\n",
        "  obs_tm1 = timestep.observation\n",
        "\n",
        "  # Sample initial action.\n",
        "  rng, policy_rng = jax.random.split(rng)\n",
        "  a_tm1 = epsilon_greedy_policy(parameters, policy_rng, obs_tm1)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Step environment.\n",
        "    new_timestep = env.step(int(a_tm1))\n",
        "\n",
        "    # Sample action from agent policy.\n",
        "    rng, policy_rng = jax.random.split(rng)\n",
        "    a_t = epsilon_greedy_policy(parameters, policy_rng, new_timestep.observation)\n",
        "\n",
        "    # Update params.\n",
        "    r_t = new_timestep.reward\n",
        "    discount_t = discount_factor * new_timestep.discount\n",
        "\n",
        "    dJ_dtheta = compute_gradient(\n",
        "        parameters, obs_tm1, a_tm1, r_t, discount_t,\n",
        "        new_timestep.observation)\n",
        "    updates, opt_state = opt_update(dJ_dtheta, opt_state)\n",
        "    parameters = apply_updates(parameters, updates)\n",
        "\n",
        "    # Within episode book-keeping.\n",
        "    episode_return += new_timestep.reward\n",
        "    timestep = new_timestep\n",
        "    obs_tm1 = new_timestep.observation\n",
        "    a_tm1 = a_t\n",
        "\n",
        "  # Experiment results tracking.\n",
        "  all_episode_returns.append(episode_return)\n",
        "\n",
        "# Plot learning curve.\n",
        "plot_learning_curve(all_episode_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g80tBzfsBgya"
      },
      "source": [
        "### Q8 [6 marks]\n",
        "\n",
        "[2pts]: What is the best asymptotic average return that can be achieved by the actor-critic agent described in `Part 2A)`? Can this be futher improved, and if so: how? Explain your answer.\n",
        "\n",
        "Note that as specified in the code, the environment of the experiment is \"Catch\" as stated in the \"B-Suite\", where reward = +1 if the paddle catches the ball (success), and reward = -1 if the paddle does not catch the ball (fail) - this is taken from the github repository: https://github.com/deepmind/bsuite/blob/master/bsuite/environments/catch.py. Assuming that the actor-critic agent works, it will learn to act in such a way that it succeeds all the time picking the optimal action, such that reward is always +1 (paddle always catches the ball). Therefore, in the asymptotic case, for infinite episodes, regardless of the initial rewards when the agent starts learning (they become negligible when compared to infinite rewards), the average return will be converging to +1. In that case, the stochastic policy will comverge to a deterministic one, where the optimal action has probability of 1, and the rest of the actions have probability 0. This cannot be further improved.\n",
        "\n",
        "[2pts] What is the best asymptotic average return that can be achieved by the second agent described in `Part 2B)`? Can this be futher improved, and if so: how? Explain your answer.\n",
        "\n",
        "In this case we follow an ϵ-greedy policy instead of a stochastic policy. Therefore, after training and assuming we have found the optimal action, at the end of an episode we could take the optimal action with probability (1-ϵ) or take a random action (including the optimal one) with probability ϵ. From the Catch experiment description, we see that there are 3 actions (paddle moves left, right, or stays still). The asymptotic average return would end up being: [(1-ϵ) * (+1)] + [ϵ * (1/3 * (+1) + 1/3 * (-1) + 1/3 * (-1))], where the first part of the sum is the greedy action, and the second part is the random action, with all 3 actions having equal probability of 1/3 of being chosen. Substituting ϵ = 0.1, we end up with an averagge return of 13/15 = 0.867. This is smaller than +1 due to the fact that we are still exploring once we've found the optimal action, so on the penultiumate state, there is a probability for the agent to choose a sub-optimal action and fail the experiment and get a reward of -1. A way to improve this, would be to use an ϵ that varies with time-steps, so that it becomes smaller with time, so that we explore less and less every time, because we're already converging to an optimal action, so that at high time-steps we would always choose the greedy action and asymptotic reward would be +1. As shown in Notebook 1 of this coursework, a good expression for ϵ could be: ϵ = 1/$\\sqrt{t}$.\n",
        "\n",
        "[2pts] What quantity do the preferences `p` estimate in the second agent described in Part B?\n",
        "\n",
        "The preferences estimate the action value function of the ϵ-greedy policy: q(s,a)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BJrDzXJxTl7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "UCL RL assignment 2022, part III",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1FmDlMclrG5eY-YjKtD8aGk0VacDzO4fm",
          "timestamp": 1645466985736
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "989a8ee0261a415be34d4cf0f45e98134ff6fbcaa2e29b3efcaef888d322ba01"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
